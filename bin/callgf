#!/usr/bin/env python

import matplotlib
matplotlib.use('Agg')

# stdlib imports
import argparse
import os.path
import sys
import glob
from datetime import datetime
import logging
import logging.config
import sqlite3
from collections import OrderedDict
import json
import re
import shutil

# third party imports
#from impactutils.io.cmd import get_command_output
from impactutils.comcat.query import GeoServe
from mapio.shake import getHeaderData
from mapio.shake import ShakeGrid
import configobj
import fiona
from shapely.geometry import shape, box
#from mpl_toolkits.basemap import Basemap

# local imports
from gfail.gfailrun import run_gfail
from gfail.transfer import gf_transfer

# config parameters required for this program to run
REQUIRED_CONFIG = ['log_filepath', 'output_filepath',
                   'trimfile', 'dbfile', 'pdl_config']

# what are the networks from which we will accept ShakeMap products?
WHITELIST = ['us', 'ci', 'nc', 'nn', 'hv', 'uw', 'nn', 'uu', 'ak']

# what are the networks that have origins but not ShakeMaps
NO_SHAKEMAPS = ['mb', 'ecx', 'tul', 'ismp',
                'nm', 'se', 'ogso', 'pr', 'neic', 'ld', 'wy', 'hv']

# minimum magnitude to be processed
MAG_THRESHOLD = 4.0

# logging configuration - sets up a rotating log file
LOG_CFG = {
    'version': 1,
    'disable_existing_loggers': False,
    'formatters': {
        'standard': {
            'format': ('%(levelname)s -- %(asctime)s -- '
                       '%(module)s.%(funcName)s -- %(message)s'),
            'datefmt': '%Y-%m-%d %H:%M:%S',
        },
    },
    'handlers': {
        'default': {
            'level': 'INFO',
            'formatter': 'standard',
            'class': 'logging.handlers.TimedRotatingFileHandler',
            'when': 'midnight',
        },
    },
    'loggers': {
        '': {
            'handlers': ['default'],
            'level': 'INFO',
            'propagate': False
        },
    }
}

# name of the logfile, will get archived at midnight
LOGFILE = 'groundfailure.log'

# name of the global configuration file
CONFIG_FILE = '.gfail_defaults'

# string time format to stuff in the database
TIMEFMT = '%Y-%m-%d %H:%M:%S'

# schema definition of the event table in the sqlite database
EVENT = OrderedDict([('id', 'INTEGER PRIMARY KEY'),
                     ('eventcode', 'TEXT'),
                     ('note', 'TEXT'),
                     ('eventdir', 'TEXT'),
                     ('version', 'INTEGER'),
                     ('shakemap_version', 'INTEGER'),
                     ('lat', 'REAL'),
                     ('lon', 'REAL'),
                     ('depth', 'REAL'),
                     ('time', 'TIMESTAMP'),
                     ('mag', 'REAL'),
                     ('location', 'TEXT'),
                     ('starttime', 'TIMESTAMP'),
                     ('endtime', 'TIMESTAMP'),
                     ('finitefault', 'INTEGER'),
                     ('HaggLS', 'REAL'),
                     ('ExpPopLS', 'REAL'),
                     ('HaggLQ', 'REAL'),
                     ('ExpPopLQ', 'REAL')])


def get_next_version(eventdir):
    """Get the number for the next version for the input event directory.

    Args:
        eventdir (str): Path to event directory.

    Returns:
        int - Number of next version of groundfailure product.

    """
    vfolders = glob.glob(os.path.join(eventdir, 'version.*'))
    if not len(vfolders):
        return 1
    vfolders = sorted(vfolders)
    lastfold = os.path.basename(vfolders[-1])
    old_version = int(re.search('[0-9]+', lastfold).group())
    version = old_version + 1
    return version


def shakemap_over_land(landfile, grid):
    """Test to see if any portion of the ShakeMap grid is over land.

    Args:
        landfile (str): Path to global shapefile of land masses.
        grid (dict): grid containing fields:
                     - lon_min
                     - lon_max
                     - lat_min
                     - lat_max
    Returns:
        bool: True if over land, False if not.
    """
    # Check if ShakeMap is entirely in water
    on_land = False

    box_tuple = (grid['lon_min'], grid['lat_min'],
                 grid['lon_max'], grid['lat_max'])
    grid_box = box(*box_tuple)

    with fiona.open(landfile, 'r') as shapefile:
        tfeatures = shapefile.items(bbox=box_tuple)
        features = [shape(feature[1]["geometry"]) for feature in tfeatures]

    for feature in features:
        if feature.intersects(grid_box):
            on_land = True
            break

    return on_land


def connect_database(dbfile):
    """Connect to the sqlite database, create if necessary.

    Args:
        dbfile (str): Path to sqlite database file.
    Returns:
        connection: SQLITE connection object.
        cursor: SQLITE cursor object.
    """

    # Create the database if it doesn't exist
    db_exists = os.path.isfile(dbfile)
    connection = None
    cursor = None
    connection = sqlite3.connect(dbfile)
    with connection:
        cursor = connection.cursor()
        if not db_exists:
            createcmd = 'CREATE TABLE shakemap ('
            nuggets = []
            for column, ctype in EVENT.items():
                nuggets.append('%s %s' % (column, ctype))
            createcmd += ','.join(nuggets) + ')'
            cursor.execute(createcmd)

    return connection


def get_version_dir(eventid, eventdir, config):
    """Get the version directory for an event.

    Args:
        eventid (str): Event ID.
        eventdir (str): Path to (possibly non-existent) event directory.
        config (ConfigObj): Configuration object.

    Returns:
        str: Path to version dictionary.
        int: Version number.
        str: Event directory.
    """
    # if eventdir is None, create a new one
    if eventdir is None:
        tnow = datetime.utcnow().strftime('%Y%m%d%H%M%S')
        eventdir = os.path.join(
            config['output_filepath'], '%s_%s' % (eventid, tnow))
        if os.path.isdir(eventdir):  # how did this happen??
            version = get_next_version(eventdir)
        else:
            os.makedirs(eventdir)
            version = 1
    else:
        if not os.path.isdir(eventdir):  # this shouldn't happen either...
            os.makedirs(eventdir)
            version = 1
        else:
            version = get_next_version(eventdir)

    # create a version directory
    vdir = os.path.join(eventdir, 'version.%03i' % version)
    return (vdir, version, eventdir)


def get_event_dir(args, connection):
    """Scan the database for existing event directory.

    Args:
        args (arparser Namespace): Input arguments.
        connection (sqlite connection): Database connection object.

    Returns:
        int: Database ID of event (-1 if not found).
        str: Event directory (None if not found).
    """
    # get all the event IDS associated with this event, then we can scan the
    # database to see if we have processed this event before.
    eventdir = None
    db_id = -1
    eventids = args.eventids.split(',')
    for eventid in eventids:
        with connection:
            cursor = connection.cursor()
            cursor.execute('SELECT id, eventdir FROM shakemap WHERE eventcode=?', (eventid,))
            row = cursor.fetchone()
        if row is None:
            continue
        db_id, eventdir = row
        break

    return (db_id, eventdir)


def insert_event(connection, eventid, args, version):
    """Insert known event information into database.

    Args:
        connection (sqlite connection): connection object.
        cursor (sqlite cursor): cursor object.
        eventid (str): Event ID.
        args (arparser Namespace): Input arguments
        version (int): Groundfailure version number.
    Returns:
        int: Database ID of event row inserted.
    """

    vfmt = ("%s", "%i", "%.4f", "%.4f", "%s", "%.1f", "%s", "%s", "%.1f")
    tnowstr = datetime.utcnow().strftime(TIMEFMT)
    if args.time is None:
        time = 'unknown'
    else:
        time = args.time
    if args.depth is None:
        depth = ''
    else:
        depth = args.depth

    tpl = (eventid, version,
           args.latitude, args.longitude, time,
           args.magnitude, tnowstr,
           'Currently running...', depth)
    list1 = []
    for fm, tp in zip(vfmt, tpl):
        list1.append(fm % tp)

    with connection:
        cursor = connection.cursor()
        cursor.execute("""INSERT INTO shakemap (eventcode, version, lat,
                       lon, time, mag, starttime, note, depth)
                       VALUES (?, ?, ?, ?, ?, ?, ?, ?, ?)""",
                       (list1))
        db_id = cursor.lastrowid

    return db_id


def update_event_fail(connection, db_id, msg, deletedir=None):
    """Update event that failed to pass later filters. Print error messages in eventdir field.
    Closes the database after.

    Args:
        connection (sqlite connection): connection object.
        cursor (sqlite cursor): cursor object.
        db_id (int): database id for current event
        msg (str): error message
        deletedir (str): directory to remove upon failure
    """
    tnowstr = datetime.utcnow().strftime(TIMEFMT)
    with connection:
        cursor = connection.cursor()
        cursor.execute('UPDATE shakemap SET endtime = ?, note = ? WHERE id = ?',
                       (tnowstr, msg, db_id))
    # Close the database
    connection.close()

    # Delete directory, if necessary
    if deletedir is not None:
        shutil.rmtree(deletedir)


def process_shakemap(args, config):
    """Process the ShakeMap.

    Args:
        args (arparser Namespace): Input arguments.
        config (ConfigObj): Configuration object.
    """
    logging.info('###### Event source: %s' % args.preferred_eventsource)
    logging.info('###### Event source code: %s' %
                 args.preferred_eventsourcecode)
    logging.info('args: \n%s' % str(args))

    tnowstr = datetime.utcnow().strftime(TIMEFMT)
    logging.info('Starting run at %s' % (tnowstr,))

    if args.status == 'DELETE':
        # look at this with respect to archiving and sending cancel messages
        msg = 'No action to take with delete messages.'
        logging.info(msg)
        if not args.force:
            sys.exit(1)

    logging.info('Checking if status is update...')
    if args.status != 'UPDATE':
        msg = 'No action to take with %s messages.' % args.status
        logging.info(msg)
        if not args.force:
            sys.exit(1)
    logging.info('Status is update. Check passed.')

    logging.info('Checking action...')
    if args.action not in ('EVENT_ADDED', 'EVENT_UPDATED',
                           'PRODUCT_ADDED', 'PRODUCT_UPDATED'):
        msg = 'No action to take with %s messages.' % args.action
        logging.info(msg)
        if not args.force:
            sys.exit(1)
    else:
        logging.info('Action check passed.')

    logging.info('Checking magnitude...')
    if args.magnitude < MAG_THRESHOLD:
        msg = ('Input ShakeMaps must be larger than M%.1f. Exiting.'
               % MAG_THRESHOLD)
        logging.info(msg)
        if not args.force:
            sys.exit(1)
    else:
        logging.info('Magnitude check passed')

    # ----------------------------------------------------------------------
    # passed initial filters, enter in database before next set of filters
    # ----------------------------------------------------------------------

    try:
        # connect to the database, creating it if necessary
        dbfile = config['dbfile']
        conn = connect_database(dbfile)

        # get the database ID and directory of the event we have
        # db_id = -1 and eventdir is None if event not in database
        db_id, eventdir = get_event_dir(args, conn)

        # get the event ID for this event
        eventsource = args.preferred_eventsource
        eventcode = args.preferred_eventsourcecode
        eventid = eventsource + eventcode

        # Get the directory for this new version
        vdir, version, eventdir = get_version_dir(eventid, eventdir, config)

        # insert what we know about the event into the database
        db_id = insert_event(conn, eventid, args, version)
        logging.info('Inserted event placeholder into database')
    except Exception as e:
        logging.critical('Database entry failed: %s' % e)
        logging.info('Continuing to run without database.')

    # ----------------------------------------------------------------------
    # Additional filters for things we'd want to know about in database file
    # ----------------------------------------------------------------------

    logging.info('Checking gridfile...')
    gridfile = os.path.join(args.directory, 'download', 'grid.xml')
    if not os.path.isfile(gridfile):
        gridfile = os.path.join(args.directory, 'grid.xml')
        if not os.path.isfile(gridfile):
            msg = ('Could not find input ShakeMap grid file at %s. Exiting.'
                   % gridfile)
            logging.info(msg)
            if not args.force:
                update_event_fail(conn, db_id, msg)
                sys.exit(1)

    hdrtuple = getHeaderData(gridfile)
    grid = hdrtuple[2]
    event = hdrtuple[1]

    try:
        with conn:
            cursor = conn.cursor()
            cursor.execute("UPDATE shakemap SET shakemap_version = ? WHERE id = ?",
                           (hdrtuple[0]['shakemap_version'], db_id))
        logging.info('Found gridfile for shakemap version %i' % hdrtuple[0]['shakemap_version'])
    except Exception as e:
        logging.critical(e)

    logging.info('Checking if Shakemap extends outside of latitude range of '
                 'slope data ...')
    shake_bounds = ShakeGrid.load(gridfile, adjust='res').getBounds()
    lat_min = shake_bounds[2]
    lat_max = shake_bounds[3]
    if (lat_min < -56.0000) or (lat_max > 84.0000):
        msg = ('Shakemap extends outside of latitude range of slope data. '
               'Exiting.')
        logging.info(msg)
        if not args.force:
            update_event_fail(conn, db_id, msg)
            sys.exit(1)
    logging.info('Shakemap bounds check passed.')

    logging.info('Checking source...')
    if args.preferred_eventsource not in WHITELIST:
        msg = ('Input ShakeMaps must be from an approved list of sources: %s. '
               'Exiting.' % str(WHITELIST))
        logging.info(msg)
        if not args.force:
            update_event_fail(conn, db_id, msg)
            sys.exit(1)
    logging.info('Shakemap from approved source list. Check passed.')

    # Use lat,lon and source to exclude ShakeMap providers who are playing
    # outside their sandbox
    logging.info('Checking for authoritativeness...')

    # we'll default to letting the event run if we can't figure out who's
    # authoritative
    authtype = 'anss'
    authsrc = args.preferred_eventsource
    try:
        gs = GeoServe(args.preferred_latitude, args.preferred_longitude)
        authsrc, authtype = gs.getAuthoritative()
    except Exception as e:
        logging.info('Unable to connect to GeoServe, assuming "%s" is '
                     'authoritative.' % args.preferred_eventsource)
    if authtype != 'anss':
        authsrc = 'us'
    is_authoritative = authsrc.lower() == args.preferred_eventsource.lower()
    if not is_authoritative and authsrc.lower() not in NO_SHAKEMAPS:
        msg = ('Source %s is not authoritative for this region (%s is).'
               % (args.preferred_eventsource, authsrc))
        logging.info(msg)
        if not args.force:
            update_event_fail(conn, db_id, msg)
            sys.exit(1)
    logging.info('Source is authoritative. Check passed.')

    logging.info('Checking to see if ShakeMap is over any land...')
    landfile = config['trimfile']
    if not shakemap_over_land(landfile, grid):
        msg = 'Input ShakeMap is completely over water. Exiting.'
        logging.info(msg)
        if not args.force:
            update_event_fail(conn, db_id, msg)
            sys.exit(1)
    else:
        logging.info('Shakemap is over land. Check passed.')

    # ----------------------------------------------------------------------
    # passed initial filters, enter in database before next set of filters
    # ----------------------------------------------------------------------

    # Make directory since it's going to run
    os.makedirs(vdir)
    logging.info('Initial filters passed, directory created: %s' % vdir)

    # ----------------------------------------------------------------------
    # We've passed all the filters, so call gfail
    # ----------------------------------------------------------------------

    # File containing list of model configs to include
    model_file = os.path.join(config['data_path'], 'autogf_models')

    logging.info("Starting run_gfail")
    pargs = {'hdf5': True,
             'gis': True,
             'make_webpage': True,
             'trimfile': landfile,
             'set_default_paths': False,
             'list_default_paths': False,
             'reset_default_paths': False,
             'shakefile': gridfile,
             'make_static_pngs': False,
             'make_static_pdfs': False,
             'make_interactive_plots': False,
             'extract_contents': True,
             'config': model_file,
             'set_bounds': None,
             'make_summary': False,
             'finite_fault': None,
             'uncertfile': None,
             'save_inputs': False,
             'std': 1.0,
             'appendname': None,
             'data_path': config['data_path'],
             'output_filepath': vdir,
             'config_filepath': config['config_filepath'],
             'mapconfig': config['mapconfig'],
             'mapdata_filepath': config['mapdata_filepath'],
             'web_template': config['web_template'],
             'popfile': config['popfile'],
             'dry_run': args.dry_run,
             }

    try:
        outfiles = run_gfail(pargs)
        if outfiles is None:
            logging.critical("run_gfail failed - no files created.")
            sys.exit(1)
    except Exception as e:
        msg = 'Failure running run_gfail: "%s"' % (str(e))
        logging.critical(msg)
        update_event_fail(conn, db_id, msg)
        sys.exit(1)
    logging.info('run_gfail completed')

    # Transfer
    logging.info("Starting gfail_transfer")
    pdl_conf_file = config['pdl_config']
    # dry = ""
    # if args.dry_run:
    #     dry = True
    success = gf_transfer(vdir, pdl_conf_file, dry_run=args.dry_run)
    if not success:
        msg = "gf_transfer failed."
        logging.crtical(msg)
        update_event_fail(conn, db_id, msg)
        sys.exit(1)
    logging.info('gf_transfer completed')

    # Enter model results into the database.
    jsonfile = os.path.join(vdir, 'info.json')
    infodata = json.load(open(jsonfile, 'rt'))
    landslides = infodata['Landslides']
    landslide = None
    for landslide in landslides:
        if landslide['preferred']:
            break
    ls_hazard_value = landslide['hazard_alert']['value']
    ls_pop_value = landslide['population_alert']['value']
    liquefactions = infodata['Liquefaction']
    liquefaction = None
    for liquefaction in liquefactions:
        if liquefaction['preferred']:
            break

    lq_hazard_value = liquefaction['hazard_alert']['value']
    lq_pop_value = liquefaction['population_alert']['value']
    #values = '(endtime,finitefault,HaggLS,ExpPopLS,HaggLQ,ExpPopLQ)'
    tnowstr = datetime.utcnow().strftime(TIMEFMT)
    finite_fault = not infodata['Summary']['point_source']

    with conn:
        cursor = conn.cursor()
        try:
            cursor.execute("UPDATE shakemap SET endtime = ?, finitefault = ?, HaggLS = ?,"
                           "ExpPopLS = ?, HaggLQ = ?, ExpPopLQ = ?, eventdir = ?,"
                           "time = ?, depth = ?, location = ?, note = ? WHERE id = ?",
                           (tnowstr, '%i' % finite_fault,
                            '%.3f' % ls_hazard_value,
                            '%.3f' % ls_pop_value,
                            '%.3f' % lq_hazard_value,
                            '%.3f' % lq_pop_value,
                            eventdir,
                            event['event_timestamp'].strftime(TIMEFMT),
                            '%.1f' % event['depth'],
                            event['event_description'],
                            '',
                            '%i' % db_id))
            logging.info('Final event information entered in database.')
        except Exception as e:
            logging.exception('Final database entry failed: %s' % e)

    # close the database
    conn.close()
    logging.info('Completed gfail run of %s' % eventid)


def main(args, config):
    """Main entry point method.

    Args:
        args (arparser Namespace): Input arguments.
        config (ConfigObj): Configuration object.

    """
    # set up a daily rotating file handler logger
    log_path = config['log_filepath']
    if not os.path.isdir(log_path):
        os.makedirs(log_path)
    logfile = os.path.join(log_path, LOGFILE)
    log_cfg = LOG_CFG.copy()
    if args.debug:
        log_cfg['handlers']['default']['class'] = 'logging.StreamHandler'
        del log_cfg['handlers']['default']['when']
    else:
        log_cfg['handlers']['default']['filename'] = logfile
    logging.config.dictConfig(log_cfg)
    logger = logging.getLogger()
    logging.info('---------------------------------------------------------')
    logging.info('Running process_shakemap')

    if args.type == 'shakemap':
        process_shakemap(args, config)
    else:
        logging.info('Incorrect type specified, exiting.')
    sys.exit(0)


if __name__ == '__main__':
    desc = """Call the groundfailure program with arguments from PDL.

This program is meant to be called by a PDL process, and generally not called
by a user, unless that user is a developer debugging callgf itself.

This program assumes that there is a configuration file in the user's home
directory called .gfail_defaults, which contains at least the following
entries:

log_filepath = A directory where rotating log files will be written.
output_filepath = A directory where event sub-directories will be created.
trimfile = Path to a shapefile containing GADM shapefile of country landmasses.
dbfile = Path to a SQLITE file containing event and version run information.
data_path = Path to model input data

A file called "autogf_models" that lists the models to run must be placed in the
data_path directory.
"""
    argparser = argparse.ArgumentParser(
        description=desc,
        formatter_class=argparse.RawDescriptionHelpFormatter)
    argparser.add_argument("--directory",
                           help="Directory where ShakeMap data can be found",
                           metavar='DIRECTORY')
    argparser.add_argument("--type",
                           help="Product type", metavar='TYPE')
    argparser.add_argument("--preferred-eventsourcecode",
                           help="Product code", metavar='CODE')
    argparser.add_argument("--preferred-eventsource",
                           help="Product source", metavar='SOURCE')
    argparser.add_argument("--status",
                           help="Product status", metavar='STATUS')
    argparser.add_argument('-d', '--debug', action='store_true',
                           default=False,
                           help='Print log messages to the screen.')
    argparser.add_argument("--action",
                           help="Product action", metavar='ACTION')
    argparser.add_argument("--preferred-latitude", type=float,
                           help="Event latitude", dest='latitude')
    argparser.add_argument("--preferred-longitude", type=float,
                           help="Event longitude", dest='longitude')
    argparser.add_argument("--preferred-depth", type=float,
                           help="Event depth", dest='depth')
    argparser.add_argument("--preferred-magnitude", type=float,
                           help="Event magnitude", metavar='MAG',
                           dest='magnitude')
    argparser.add_argument("--preferred-eventtime",
                           help="Event time", metavar='TIME', dest='time')
    argparser.add_argument("--eventids", help="List of associated event IDs")
    argparser.add_argument("--dry-run",
                           action='store_true', default=False,
                           help="Do not transfer result to PDL.")
    argparser.add_argument("-f", "--force",
                           action='store_true', default=False,
                           help="Force ground-failure run; ignore "
                                "authoritativeness checks.")

    pargs, unknown = argparser.parse_known_args()

    # make sure the config file is where we expect it to be, and read it
    config_file = os.path.join(os.path.expanduser('~'), CONFIG_FILE)
    pconfig = configobj.ConfigObj(config_file)

    if not set(REQUIRED_CONFIG).issubset(set(list(pconfig.keys()))):
        fmt = 'Missing some of the required entries in %s. Needed:'
        print(fmt % config_file)
        for req in REQUIRED_CONFIG:
            print('\t%s' % req)
        sys.exit(1)

    main(pargs, pconfig)
